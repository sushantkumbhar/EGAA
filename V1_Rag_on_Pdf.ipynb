{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO1CgSqtbS5X"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga8uDQ9DbS5Y"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2 --quiet\n",
        "!pip install keybert --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDih7YK5bS5Z"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ4QNhfubS5a"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UUEBJG0bS5b"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_qJBZR3bS5b"
      },
      "outputs": [],
      "source": [
        "# for text preprocessing\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# import vectorizers\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RF_xu_ybS5c"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esLibZwBbS5c"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkZTi_OybS5c",
        "outputId": "851860fa-ff66-43a1-fc53-5f92074b2d6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "# import wordnet\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpjSxn2CbS5d"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Phrases\n",
        "from gensim.corpora import Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJKkEfIobS5d"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMdQpvLZbS5d"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rreJNsZebS5e"
      },
      "outputs": [],
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOx4op3kbS5e"
      },
      "outputs": [],
      "source": [
        "from gensim.models import LdaModel, LdaMulticore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW8U0GPObS5e"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIp1S8ZebS5f"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D6soBxfbS5f"
      },
      "outputs": [],
      "source": [
        "# !pip install flair\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj7wY4rnbS5f",
        "outputId": "42ac63b4-9237-45aa-9087-2776eb22d273"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "from keybert import KeyBERT\n",
        "from keybert.backend import BaseEmbedder\n",
        "# from flair.embeddings import TransformerDocumentEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6t_7oZsbS5g"
      },
      "outputs": [],
      "source": [
        "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "\n",
        "# tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "# query_model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_up5-mAbS5g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4VeMzm9bS5g"
      },
      "outputs": [],
      "source": [
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_nxvnWmbS5h",
        "outputId": "b7fa730a-f165-4817-da2d-9d0447e85cee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWi6Qt7cbS5h",
        "outputId": "ff7a052d-8f12-44be-c055-e07e26102f3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kODJTzpCbS5h"
      },
      "outputs": [],
      "source": [
        "import faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2K-w0wWbS5h"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArQHqHBBbS5h"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fam0hhHAa9tf",
        "outputId": "a70b91d8-f4cb-4dc8-d355-24c6c902a3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.38)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.111)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: langchain-text-splitters, langchain\n",
            "Successfully installed langchain-0.2.16 langchain-text-splitters-0.2.4\n",
            "Collecting ctransformers\n",
            "  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from ctransformers) (0.23.5)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from ctransformers) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers) (2024.7.4)\n",
            "Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ctransformers\n",
            "Successfully installed ctransformers-0.2.27\n",
            "Requirement already satisfied: ctransformers[cuda] in /usr/local/lib/python3.10/dist-packages (0.2.27)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from ctransformers[cuda]) (0.23.5)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from ctransformers[cuda]) (9.0.0)\n",
            "Collecting nvidia-cublas-cu12 (from ctransformers[cuda])\n",
            "  Downloading nvidia_cublas_cu12-12.6.1.4-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12 (from ctransformers[cuda])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers[cuda]) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[cuda]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[cuda]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[cuda]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers[cuda]) (2024.7.4)\n",
            "Downloading nvidia_cublas_cu12-12.6.1.4-py3-none-manylinux2014_x86_64.whl (378.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.9/378.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu12, nvidia-cublas-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.6.1.4 nvidia-cuda-runtime-cu12-12.6.68\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install ctransformers\n",
        "!pip install ctransformers[cuda]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so4hK5XJbFCC",
        "outputId": "afe6de2f-6bdb-490d-d357-d1ab59138630"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.16 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.16)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.38)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.111)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.16->langchain-community) (0.2.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.16->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.2.16-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.16 marshmallow-3.22.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YTuIoAIbKX3",
        "outputId": "f0dba60a-b1a0-470e-af97-11ced89c7974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.10/dist-packages (0.1.9)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain-groq) (0.11.0)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.26 in /usr/local/lib/python3.10/dist-packages (from langchain-groq) (0.2.38)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (0.1.111)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (8.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (3.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.0.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n",
        "!pip install langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IagvrcbjbM7d"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj2yW95JbkT8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjqrKRTPbS5i"
      },
      "source": [
        "## Paragraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZfz6vEzbS5i"
      },
      "outputs": [],
      "source": [
        "# pattern = r'fig(ure)?.?.?[0-9]+.[0-9]+'\n",
        "# re.search( string=\"This is a figure 1.5 and it works\", flags=re.IGNORECASE, pattern=pattern)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGQjwuG7bS5i"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "MODEL_STRING = \"facebook/dpr-ctx_encoder-single-nq-base\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rgDSVkAbS5i"
      },
      "outputs": [],
      "source": [
        "class Paragraph:\n",
        "    def __init__(self, book_id:int, id: int, text: str, embeddings: np.array, tagged_images: list=None):\n",
        "        self.text = text\n",
        "        self.embeddings = embeddings\n",
        "        self.id = id\n",
        "        self.book_id = book_id\n",
        "    def get_json(self):\n",
        "        dc = {\n",
        "            \"text\": self.text,\n",
        "            \"id\": self.id,\n",
        "            \"embeddings\": self.embeddings\n",
        "        }\n",
        "        return json.dumps(dc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z59sueVibS5i"
      },
      "outputs": [],
      "source": [
        "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQh079EHaLAB"
      },
      "outputs": [],
      "source": [
        "class EncoderModels():\n",
        "    def __init__(self):\n",
        "        ctx_model_string = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
        "        query_model_string = \"facebook/dpr-question_encoder-single-nq-base\"\n",
        "\n",
        "        DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = DEVICE\n",
        "\n",
        "        self.ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(ctx_model_string)\n",
        "        self.ctx_model = DPRContextEncoder.from_pretrained(ctx_model_string)\n",
        "        self.ctx_model = self.ctx_model.to(DEVICE)\n",
        "\n",
        "        self.query_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(query_model_string)\n",
        "        self.query_model = DPRQuestionEncoder.from_pretrained(query_model_string)\n",
        "        self.query_model = self.query_model.to(DEVICE)\n",
        "\n",
        "    def encode_context(self, paragraph: list[str],):\n",
        "        num_paragraphs = len(paragraph)\n",
        "        cloned_doc_embeddings = []\n",
        "        batch_size = 32\n",
        "        # print(paragraph)\n",
        "        for itr in range(0, num_paragraphs, batch_size):\n",
        "            s = itr\n",
        "            e = min(itr+batch_size, num_paragraphs)\n",
        "            tokenized = self.ctx_tokenizer(paragraph[s : e],\n",
        "                                       padding=True, truncation=True,\n",
        "                                       return_tensors=\"pt\",\n",
        "                                       return_attention_mask=True,\n",
        "                                       add_special_tokens=True,\n",
        "                                       max_length=512,\n",
        "                                       pad_to_max_length=True\n",
        "                                       ).to(self.device)\n",
        "            input_ids = tokenized['input_ids']\n",
        "            print(input_ids.shape)\n",
        "            attention_mask = tokenized['attention_mask']\n",
        "            print(attention_mask)\n",
        "            with torch.no_grad():\n",
        "                doc_embeddings = self.ctx_model(input_ids, attention_mask=attention_mask).pooler_output\n",
        "\n",
        "            cloned_doc_embeddings.append(torch.clone(doc_embeddings).cpu().numpy())\n",
        "\n",
        "            ##clean up\n",
        "            torch.cuda.empty_cache()\n",
        "            del input_ids\n",
        "\n",
        "        return np.vstack(cloned_doc_embeddings)\n",
        "    def encode_query(self, query):\n",
        "\n",
        "        tokenized = self.query_tokenizer(query,\n",
        "                                    padding=True, truncation=True,\n",
        "                                    return_tensors=\"pt\",\n",
        "                                    return_attention_mask=True,\n",
        "                                    add_special_tokens=True,\n",
        "                                    max_length=512,\n",
        "                                    pad_to_max_length=True\n",
        "                                    ).to(self.device)\n",
        "        input_ids = tokenized['input_ids']\n",
        "        attention_mask = tokenized['attention_mask']\n",
        "        with torch.no_grad():\n",
        "            doc_embeddings = self.query_model(input_ids, attention_mask=attention_mask).pooler_output\n",
        "        embedding = torch.clone(doc_embeddings).cpu().numpy()\n",
        "\n",
        "        ##clean up\n",
        "        torch.cuda.empty_cache()\n",
        "        del input_ids\n",
        "\n",
        "        return embedding\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy2ARVuDbS5j"
      },
      "outputs": [],
      "source": [
        "class EncoderModel():\n",
        "    def __init__(self, model_string: str = MODEL_STRING, device: str= DEVICE):\n",
        "        super().__init__()\n",
        "        self.device = DEVICE\n",
        "        self.model_string = model_string\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_string)\n",
        "        self.model = AutoModel.from_pretrained(model_string, resume_download=True)\n",
        "        self.model = self.model.to(DEVICE)\n",
        "        # self.tokenizer = self.tokenizer.to(self.device)\n",
        "\n",
        "\n",
        "    def encode(self, paragraph: list[str],):\n",
        "        num_paragraphs = len(paragraph)\n",
        "        cloned_doc_embeddings = []\n",
        "        batch_size = 32\n",
        "        print(paragraph)\n",
        "        for itr in range(0, num_paragraphs, batch_size):\n",
        "            s = itr\n",
        "            e = min(itr+batch_size, num_paragraphs)\n",
        "            tokenized = self.tokenizer(paragraph[s : e],\n",
        "                                       padding=True, truncation=True,\n",
        "                                       return_tensors=\"pt\",\n",
        "                                       return_attention_mask=True,\n",
        "                                       add_special_tokens=True,\n",
        "                                       max_length=512,\n",
        "                                       pad_to_max_length=True\n",
        "                                       ).to(self.device)\n",
        "            input_ids = tokenized['input_ids']\n",
        "            print(input_ids.shape)\n",
        "            attention_mask = tokenized['attention_mask']\n",
        "            print(attention_mask)\n",
        "            with torch.no_grad():\n",
        "                doc_embeddings = self.model(input_ids, attention_mask=attention_mask).pooler_output\n",
        "\n",
        "            cloned_doc_embeddings.append(torch.clone(doc_embeddings).cpu().numpy())\n",
        "\n",
        "            ##clean up\n",
        "            torch.cuda.empty_cache()\n",
        "            del input_ids\n",
        "\n",
        "        return np.vstack(cloned_doc_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXTjDLBLbS5j"
      },
      "outputs": [],
      "source": [
        "# encoder = Encoder_Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgdSaaAzbS5j"
      },
      "outputs": [],
      "source": [
        "# emb = encoder.encode([\"Its been a long night\",\"tomowwor we win this game by hige margin\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XVIvNXbbS5j"
      },
      "outputs": [],
      "source": [
        "# emb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY3AkGzabar6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBMKuZBPbUhu"
      },
      "source": [
        "## Pdf Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za1G4F4KbqOk"
      },
      "outputs": [],
      "source": [
        "class PdfParagraphExtractor:\n",
        "    def __init__(self, book_path: str) -> None:\n",
        "        self.book_path = book_path\n",
        "\n",
        "        self.reader = PyPDF2.PdfReader(book_path)\n",
        "\n",
        "        self.paragraphs = []\n",
        "        self.paragraph_count = 0\n",
        "\n",
        "        self.extract_paragraphs()\n",
        "    def extract_paragraphs(self):\n",
        "        for page in self.reader.pages:\n",
        "            text = page.extract_text()\n",
        "            paragraphs = text.split('.\\n')\n",
        "            for paragraph in paragraphs:\n",
        "                paragraph = paragraph.replace('\\n', '').replace('\\t', '')\n",
        "                if(paragraph!='' and paragraph!=None and len(paragraph.split())>0):\n",
        "                    self.paragraphs.append(paragraph)\n",
        "        self.paragraph_count = len(self.paragraphs)\n",
        "\n",
        "    def get_paragraphs(self):\n",
        "        return self.paragraphs\n",
        "    def get_paragraph_count(self):\n",
        "        return self.paragraph_count\n",
        "    def get_paragraph(self, index: int):\n",
        "        return self.paragraphs[index]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ND2SOMcQFG"
      },
      "source": [
        "## Faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrLG0mRccRio"
      },
      "outputs": [],
      "source": [
        "class Faiss:\n",
        "    def __init__(self,doc_embeddings,model,top_k=15,dimension=768):\n",
        "        # doc embeddings is an array of vectors of shape (1,768)\n",
        "        self.doc_embeddings = np.vstack(doc_embeddings)\n",
        "        self.top_k=top_k\n",
        "        self.dimension=dimension\n",
        "        res = faiss.StandardGpuResources()\n",
        "        flat_config = faiss.GpuIndexFlatConfig()\n",
        "\n",
        "        faiss.normalize_L2(self.doc_embeddings)\n",
        "        self.index=faiss.GpuIndexFlatL2(res,self.dimension,flat_config)\n",
        "\n",
        "        self.query_model=model\n",
        "        self.create_index()\n",
        "    def create_index(self):\n",
        "        # Add your vector data to the index\n",
        "        self.index.add(self.doc_embeddings)\n",
        "        return self.index.ntotal\n",
        "    def serve_query(self,question):\n",
        "        query_embedding = self.query_model.encode_query(question)\n",
        "\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        D, I = self.index.search(query_embedding, self.top_k)\n",
        "        return D,I\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krfCaxDtcsWw"
      },
      "source": [
        "## Hypergraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_QvBswgcu-Y"
      },
      "outputs": [],
      "source": [
        "class Hypergraph():\n",
        "    # all the hypergraph related mapping and its index\n",
        "    def __init__(self, name, key_to_docids, key_to_embeddings, key_to_keyid, keyid_to_key, encoder_models):\n",
        "        self.name = name\n",
        "        self.key_to_docids = key_to_docids\n",
        "        self.key_to_embeddings = key_to_embeddings\n",
        "        self.key_to_keyid = key_to_keyid\n",
        "        self.keyid_to_key = keyid_to_key\n",
        "\n",
        "        self.vector_db = Faiss(list(self.key_to_embeddings.values()), encoder_models)\n",
        "\n",
        "    def query_hypergraph(self, question):\n",
        "        D, I = self.vector_db.serve_query(question)\n",
        "        key_ids = I[0]\n",
        "        ans_arr = []\n",
        "        for key_id in key_ids:\n",
        "            key = self.keyid_to_key[key_id]\n",
        "            docids = self.key_to_docids[key]\n",
        "            ans_arr.extend(docids)\n",
        "        return ans_arr\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GciXeyt4c0Ks"
      },
      "source": [
        "## Pdf wrapper class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfXI-MsuczbI"
      },
      "outputs": [],
      "source": [
        "class Models():\n",
        "    def __init__(self):\n",
        "        self.encoder_models = EncoderModels()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kyI_0PtdZA7"
      },
      "outputs": [],
      "source": [
        "class Book():\n",
        "    def __init__(self, book_paths, models):\n",
        "        self.book_paths = book_paths\n",
        "        self.models = models\n",
        "\n",
        "        #some global identifiers\n",
        "        self.global_paragraphs = None\n",
        "        self.global_doc_embeddings = None\n",
        "        self.pobj = None\n",
        "        self.num_paragraphs = 0\n",
        "\n",
        "        #read book\n",
        "        self.books = [PdfParagraphExtractor(book_path) for book_path in book_paths]\n",
        "        print(self.books[0].paragraphs)\n",
        "        self.bookid_to_pid = defaultdict(list)\n",
        "        self.pid_to_bookid = defaultdict(int)\n",
        "        self.set_global_info()\n",
        "\n",
        "        self.normal_hg = self.generate_normal_hg()\n",
        "\n",
        "    def get_paragraphs(self):\n",
        "        return self.global_paragraphs\n",
        "    def set_global_info(self):\n",
        "        all_ps = []\n",
        "        all_objs = []\n",
        "        all_doc_embeddings = []\n",
        "\n",
        "        id = 0\n",
        "        for bookid, book in enumerate(self.books):\n",
        "            doc_embeddings_by_book = self.compute_doc_embeddings(book.paragraphs)\n",
        "\n",
        "            for pid, paragraph in enumerate(book.paragraphs):\n",
        "                all_ps.append(paragraph)\n",
        "                all_doc_embeddings.append(doc_embeddings_by_book[pid])\n",
        "\n",
        "                self.bookid_to_pid[bookid].append(id)\n",
        "                self.pid_to_bookid[id] = bookid\n",
        "                id+=1\n",
        "\n",
        "        self.num_paragraphs = len(all_ps)\n",
        "        self.global_paragraphs = all_ps\n",
        "        self.global_doc_embeddings = all_doc_embeddings\n",
        "\n",
        "        for id in range(self.num_paragraphs):\n",
        "            all_objs.append(Paragraph(self.pid_to_bookid[id], id, all_ps[id], all_doc_embeddings[id]))\n",
        "        self.pobj = all_objs\n",
        "\n",
        "        return\n",
        "\n",
        "    def compute_doc_embeddings(self, para_list):\n",
        "        doc_embeddings = self.models.encoder_models.encode_context(para_list)\n",
        "        r, c = doc_embeddings.shape\n",
        "        doc_embeddings = [doc_emb[:].reshape(1,-1) for doc_emb in doc_embeddings]\n",
        "        return doc_embeddings\n",
        "\n",
        "    def get_docs_from_docids(self, docids):\n",
        "        ans = []\n",
        "        for docid in docids:\n",
        "            ans.append(self.global_paragraphs[docid])\n",
        "        return \" \".join(ans)\n",
        "\n",
        "    def generate_normal_hg(self):\n",
        "        n = len(self.global_paragraphs)\n",
        "        key_to_docids = {}\n",
        "        key_to_keyid = {}\n",
        "        keyid_to_key = {}\n",
        "        key_to_embeddings = {}\n",
        "        for i in range(n):\n",
        "            key_to_docids[i] = [i]\n",
        "            key_to_keyid[i] = i\n",
        "            keyid_to_key[i] = i\n",
        "            key_to_embeddings[i] = self.global_doc_embeddings[i]\n",
        "\n",
        "        normal = Hypergraph('normal', key_to_docids, key_to_embeddings, key_to_keyid, keyid_to_key, self.models.encoder_models)\n",
        "        return normal\n",
        "\n",
        "    def query_hypergraph(self, question):\n",
        "        doc_ids = self.normal_hg.query_hypergraph(question)\n",
        "        context = self.get_docs_from_docids(doc_ids)\n",
        "        return context\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Faop5qlhfmEq"
      },
      "outputs": [],
      "source": [
        "groq_api_key=\"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\"\n",
        "\n",
        "# st.title(\"Objectbox VectorstoreDB With Llama3 Demo\")\n",
        "\n",
        "llm=ChatGroq(groq_api_key=groq_api_key,\n",
        "             model_name=\"Llama3-8b-8192\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mqEU5iqfTUU"
      },
      "outputs": [],
      "source": [
        "def generate_answer(question: str, bookObj: Book):\n",
        "    extracted_text = bookObj.query_hypergraph(question)\n",
        "\n",
        "    prompt = f'Given the context: {extracted_text} /n Answer the question {question}'\n",
        "    print(prompt)\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1CZNzfkf_32"
      },
      "outputs": [],
      "source": [
        "book_paths = ['/content/test2.pdf', '/content/test.pdf']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "t6YD5OMVgDCu",
        "outputId": "031e2999-38bf-489f-bf8d-c414de8d7f62"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Models' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8e5d7f1daf72>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Models' is not defined"
          ]
        }
      ],
      "source": [
        "models = Models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VSk7g9KgH7M"
      },
      "outputs": [],
      "source": [
        "books = Book(book_paths, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce-s-rVXgNXI"
      },
      "outputs": [],
      "source": [
        "question = 'What is scope of work for MPDA?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdW98v7egN6j"
      },
      "outputs": [],
      "source": [
        "generate_answer(question, books)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKt5-ToOFkWz"
      },
      "source": [
        "## Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGw4p5f7Fm_P",
        "outputId": "17a5e0b5-1283-4430-c9b2-1be2d15107bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kye6Uz1bF73V",
        "outputId": "6dad6c0f-c5f9-42de-e26d-34d33332f7e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 3s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues, run:\n",
            "  npm audit fix\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52pDuYDRFsE3",
        "outputId": "d9d44591-3e02-41eb-d347-0e8b36a696b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "# %%writefile app.py\n",
        "\n",
        "# import streamlit as st\n",
        "\n",
        "# st.write('Hello, *World!* :sunglasses:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJgeKWvvJsj_"
      },
      "outputs": [],
      "source": [
        "def PRINT(ac):\n",
        "    print(ac)\n",
        "    return ac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHBYDWzgIDkh",
        "outputId": "f7f8a03b-5783-4e1d-c5c3-0ce8e3dddb6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import io\n",
        "import PyPDF2\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "st.title(\"Echo Bot\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"Say Something\"):\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    response = f\"Echo: {prompt}\"\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose a file\", 'pdf')\n",
        "if uploaded_file is not None:\n",
        "    # To read file as bytes:\n",
        "    bytes_data = uploaded_file.getvalue()\n",
        "    # st.write(bytes_data)\n",
        "\n",
        "    bytes_stream_data = io.BytesIO(bytes_data)\n",
        "    pdfObj = PyPDF2.PdfReader(bytes_stream_data)\n",
        "    # st.write(pdfObj)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6-JSYbJF2jL"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "elWCbR42GBoS",
        "outputId": "4c5b947b-6259-4bda-f0fa-ee659f9ef02d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "your url is: https://young-tips-march.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm7BWRvhGEtA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}